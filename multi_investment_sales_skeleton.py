# Databricks notebook source
# MAGIC %run ./util/notebook-config

# COMMAND ----------

# MAGIC %run ./util/generate-data

# COMMAND ----------

# MAGIC %md
# MAGIC # Data Description

# COMMAND ----------

# MAGIC %md
# MAGIC We create one outcome of interest:
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Revenue** | continuous | \\$ Annual revenue from customer given by the amount of software purchased
# MAGIC
# MAGIC We consider three possible treatments, the interventions whose impact we wish to measure:
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Tech Support** | binary | whether the customer received tech support during the year
# MAGIC **Discount** | binary | whether the customer was given a discount during the year
# MAGIC **New Strategy** | binary | whether the customer was targeted for a new engagement strategy with different outreach behaviors
# MAGIC
# MAGIC Finally, we consider a variety of additional customer characteristics that may affect revenue. Including these types of features is crucial for causal analysis in order to map the full causal graph and separate the true effects of treatments on outcomes from other correlation generated by other influences. 
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Global Flag** | binary | whether the customer has global offices
# MAGIC **Major Flag** | binary | whether the customer is a large consumer in their industry
# MAGIC **SMC Flag** | binary | whether the customer is a Small Medium Corporation (as opposed to large corporation)
# MAGIC **Commercial Flag** | binary | whether the customer's business is commercial (as opposed to public secor)
# MAGIC **Planning Summit** | binary | whether a sales team member held an outreach event with the customer during the year
# MAGIC **New Product Adoption** | binary | whether the customer signed a contract for any new products during the year
# MAGIC **IT Spend** | continuous | \\$ spent on IT-related purchases 
# MAGIC **Employee Count** | continuous | number of employees
# MAGIC **PC Count** | continuous | number of PCs used by the customer
# MAGIC **Size** | continuous | customer's total revenue in the previous calendar year
# MAGIC
# MAGIC In simulating the data, we maintain some key characteristics of the data from the real company example, including some correlation patterns between features and some potentially difficult data characteristics, such as large outliers.

# COMMAND ----------

# MAGIC %md
# MAGIC #Causal DAG discovery

# COMMAND ----------

from causallearn.search.ConstraintBased.PC import pc

input_df = ground_truth_df.iloc[:,0:14]
# default parameters
cg = pc(np.vstack(input_df.to_numpy()), node_names=input_df.columns, alpha=0.01)

# visualization using pydot
cg.draw_pydot_graph()

# COMMAND ----------

# Adding missing directions
add_directions(
    causal_graph=cg,
    directions=[
        {"from": "Size", "to": "IT Spend"},
        {"from": "IT Spend", "to": "Tech Support"},
        {"from": "Tech Support", "to": "New Product Adoption"},
    ],
)

# Correcting directions
invert_directions(
    causal_graph=cg,
    directions=[
        {"from": "Revenue", "to": "Commercial Flag"},
        {"from": "New Engagement Strategy", "to": "Commercial Flag"},
        {"from": "New Engagement Strategy", "to": "Commercial Flag"},
    ],
)

# Domain Knowledge derived directed relations
add_directions(
    causal_graph=cg,
    directions=[
        {"from": "Global Flag", "to": "Revenue"},
        {"from": "Major Flag", "to": "Revenue"},
        {"from": "Employee Count", "to": "PC Count"},
    ],
)

# Add effect from all basic characteristics to incentives
add_relations_influencing_incentives(
    causal_graph=cg,
    incentives=["Discount", "Tech Support", "New Engagement Strategy"],
    account_basic_characteristics=[
        "Major Flag",
        "SMC Flag",
        "Commercial Flag",
        "IT Spend",
        "Employee Count",
        "PC Count",
    ],
)


cg.draw_pydot_graph()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Influence Identification and Estimation

# COMMAND ----------

import dowhy

graph = to_pydot(cg.G, labels=input_df.columns) 

tech_support_effect_model = dowhy.CausalModel(data=input_df,
                     graph=graph,
                     treatment="Tech Support", 
                     outcome="Revenue"
                     )

tech_support_total_effect_identified_estimand = tech_support_effect_model.identify_effect(
    estimand_type="nonparametric-ate",
    method_name="maximal-adjustment",
)
print(tech_support_total_effect_identified_estimand) 

# COMMAND ----------

import mlflow

mlflow.autolog(disable=True)

model_t, model_y = setup_treatment_and_out_models()

effect_modifiers = ["Size", "Global Flag"]
method_name = "backdoor.econml.dml.LinearDML"
init_params = {
  "model_t": model_t,
  "model_y": model_y,
  "linear_first_stages": True,
  "discrete_treatment": True,
  "cv": 3,
  "mc_iters": 10,   
}


tech_support_total_effect_estimate = tech_support_effect_model.estimate_effect(
    tech_support_total_effect_identified_estimand,
    effect_modifiers=effect_modifiers, 
    method_name=method_name, 
    method_params={"init_params": init_params},
)

tech_support_total_effect_estimate.interpret()

# COMMAND ----------

tech_support_direct_effect_identified_estimand = tech_support_effect_model.identify_effect(
    estimand_type="nonparametric-cde",
    method_name="maximal-adjustment",
)
print(tech_support_direct_effect_identified_estimand) 

# COMMAND ----------


import mlflow

mlflow.autolog(disable=True)

model_t, model_y = setup_treatment_and_out_models()

effect_modifiers = ["Size", "Global Flag"]
method_name = "backdoor.econml.dml.LinearDML"
init_params = {
  "model_t": model_t,
  "model_y": model_y,
  "linear_first_stages": True,
  "discrete_treatment": True,
  "cv": 3,
  "mc_iters": 1,
}

tech_support_direct_effect_estimate = tech_support_effect_model.estimate_effect(
    tech_support_direct_effect_identified_estimand,
    effect_modifiers=effect_modifiers, 
    method_name=method_name,
    method_params={"init_params": init_params},
)

tech_support_direct_effect_estimate.interpret()

# COMMAND ----------

discount_effect_model = dowhy.CausalModel(data=input_df,
                     graph=graph,
                     treatment="Discount", 
                     outcome="Revenue"
                     )

discount_effect_identified_estimand = discount_effect_model.identify_effect(
    estimand_type="nonparametric-ate",
    method_name="maximal-adjustment",
)

print(discount_effect_identified_estimand)

# COMMAND ----------

import mlflow

mlflow.autolog(disable=True)

model_t, model_y = setup_treatment_and_out_models()

effect_modifiers = ["Size", "Global Flag"]
method_name = "backdoor.econml.dml.LinearDML"
init_params = {
  "model_t": model_t,
  "model_y": model_y,
  "linear_first_stages": True,
  "discrete_treatment": True,
  "cv": 3,
  "mc_iters": 10,
}

discount_effect_estimate = discount_effect_model.estimate_effect(
    discount_effect_identified_estimand, 
    confidence_intervals=True,
    effect_modifiers=effect_modifiers,
    method_name=method_name,
    method_params={"init_params": init_params},
)

discount_effect_estimate.interpret()

# COMMAND ----------

import dowhy

new_strategy_effect_model = dowhy.CausalModel(
    data=input_df, 
    graph=graph, 
    treatment="New Engagement Strategy", 
    outcome="Revenue"
)

new_strategy_effect_identified_estimand = new_strategy_effect_model.identify_effect(
    proceed_when_unidentifiable=True
)
print(new_strategy_effect_identified_estimand)

# COMMAND ----------

new_strategy_effect_estimate = new_strategy_effect_model.estimate_effect(
    new_strategy_effect_identified_estimand,
    method_name="backdoor.propensity_score_matching",
    target_units="att",
)
new_strategy_effect_estimate.value

# COMMAND ----------

estimates_df = pd.DataFrame(
    {
      "Estimated Direct Treatment Effect: Tech Support":[tech_support_direct_effect_estimate.value],
      "Estimated Total Treatment Effect: Tech Support":[tech_support_total_effect_estimate.value],
      "Estimated Total Treatment Effect: Discount":[discount_effect_estimate.value],
      "Estimated Total Treatment Effect: New Engagement Strategy":[new_strategy_effect_estimate.value],
    }
)

compare_estimations_vs_ground_truth(ground_truth_df, estimates_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##Refutation

# COMMAND ----------

res_random_common_cause = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="random_common_cause",
    num_simulations=100,
    n_jobs=16,
)

refutation_random_common_cause_df = pd.DataFrame([{
        "Refutation Type": res_random_common_cause.refutation_type,
        "Estimated Effect": res_random_common_cause.estimated_effect,
        "New Effect": res_random_common_cause.new_effect,
        "Refutation Result (p value)": res_random_common_cause.refutation_result["p_value"] 
    }])

refutation_random_common_cause_df  

# COMMAND ----------

import mlflow
 
mlflow.autolog(disable=True)

res_unobserved_common_cause = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="add_unobserved_common_cause",
    confounders_effect_on_treatment="binary_flip",
    confounders_effect_on_outcome="linear",
    effect_fraction_on_treatment=0.05,
    effect_fraction_on_outcome=0.05,
)

refutation_unobserved_common_cause_df = pd.DataFrame([{
        "Refutation Type": res_unobserved_common_cause.refutation_type,
        "Estimated Effect": res_unobserved_common_cause.estimated_effect,
        "New Effect": res_unobserved_common_cause.new_effect,
        "Refutation Result (p value)": None 
    }])

refutation_unobserved_common_cause_df  

# COMMAND ----------

res_placebo = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="placebo_treatment_refuter",
    placebo_type="permute",
    num_simulations=100,
    n_jobs=16,
)

refutation_placebo_df = pd.DataFrame([{
        "Refutation Type": res_placebo.refutation_type,
        "Estimated Effect": res_placebo.estimated_effect,
        "New Effect": res_placebo.new_effect,
        "Refutation Result (p value)": res_placebo.refutation_result["p_value"] 
    }])

refutation_placebo_df  

# COMMAND ----------

res_subset = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="data_subset_refuter",
    subset_fraction=0.8,
    num_simulations=100,
    n_jobs=16,
)

refutation_subset_df = pd.DataFrame([{
        "Refutation Type": res_subset.refutation_type,
        "Estimated Effect": res_subset.estimated_effect,
        "New Effect": res_subset.new_effect,
        "Refutation Result (p value)": res_subset.refutation_result["p_value"] 
    }])

refutation_subset_df  

# COMMAND ----------

import mlflow
mlflow.autolog(disable=True)

coefficients = np.array([10, 0.02])
bias = 1000


def linear_gen(df):
    y_new = np.dot(df[["W0", "W1"]].values, coefficients) + bias
    return y_new


res_dummy_outcome = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="dummy_outcome_refuter",
    outcome_function=linear_gen,
)[0]

refutation_dummy_outcome_df = pd.DataFrame([{
        "Refutation Type": res_dummy_outcome.refutation_type,
        "Estimated Effect": res_dummy_outcome.estimated_effect,
        "New Effect": res_dummy_outcome.new_effect,
        "Refutation Result (p value)": res_dummy_outcome.refutation_result["p_value"] 
    }])

refutation_dummy_outcome_df  

# COMMAND ----------

refutation_df = pd.concat([
        refutation_random_common_cause_df,
        refutation_unobserved_common_cause_df,
        refutation_subset_df,
        refutation_placebo_df,
        refutation_dummy_outcome_df,
    ])
refutation_df

# COMMAND ----------

# MAGIC %md
# MAGIC # Make Policy Recommendations

# COMMAND ----------

# MAGIC %md
# MAGIC In this section, we use EconML tools to visualize differences in conditional average treatment effects across customers and select an optimal investment plan for each customer.
# MAGIC
# MAGIC In order to decide whether to offer each investment to the customer, we need to know the cost of providing the incentive as well as the benefits of doing so. In this step we define a cost function to specify how expensive it would be to provide each kind of incentive to each customer. In other data samples you can define these costs as a function of customer features, upload a matrix of costs, or set constant costs for each treatment (the default is zero). In this example, we set the cost of ```discount``` to be a fix value of $7000 per account, while the cost of ```tech support``` is $100 per PC.

# COMMAND ----------

# MAGIC %md
# MAGIC #### Individualized policy recommendations 

# COMMAND ----------

# MAGIC %md
# MAGIC For our current sample of customers, we can also identify the best treatment plan for each individual customer based on their CATE. We use the model's `const_marginal_effect` method to find the counterfactual treatment effect for each possible treatment. We then subtract the treatment cost and choose the treatment with the highest return. That is the recommended policy.
# MAGIC
# MAGIC To visualize this output, we plot each customer based on their PC count and past revenue, the most important determinants of treatment according to the tree interpreter, and color code them based on recommended treatment.

# COMMAND ----------

import mlflow

class PersonalizedIncentiveRecommender(mlflow.pyfunc.PythonModel):
  
    def __init__(self, models_dictionary, effect_modifiers):
        self.models_dictionary = models_dictionary
        self.effect_modifiers = effect_modifiers

    def _estimate_isolated_effect(self, model_input):
        return pd.DataFrame(
            {
                f"{key} net effect": np.hstack(
                    model.const_marginal_effect(model_input[self.effect_modifiers])
                )
                for key, model in self.models_dictionary.items()
            }
        )

    def _estimate_interaction_effect(self, estimated_effects):
        effects_interaction = (
            " and ".join(self.models_dictionary.keys()) + " net effect"
        )
        estimated_effects[effects_interaction] = estimated_effects.sum(axis=1)
        return estimated_effects

    def _cost_fn_interaction(self, data):
        t1_cost = data[["PC Count"]].values * 100
        t2_cost = np.ones((data.shape[0], 1)) * 7000
        return np.hstack([t1_cost, t2_cost, t1_cost + t2_cost])

    def _estimate_net_effects(self, estimated_effects, costs):
        return estimated_effects - costs

    def _get_recommended_incentive(self, net_effects):
        net_effects["recommended incentive"]= net_effects.idxmax(axis=1).apply(
          lambda x: x.replace(" net effect", "")
          )
        net_effects["recommended incentive net effect"] = net_effects.max(axis=1)  
        return net_effects

    def predict(self, context, model_input):
        estimated_effects = self._estimate_isolated_effect(model_input)
        estimated_effects = self._estimate_interaction_effect(estimated_effects)
        costs = self._cost_fn_interaction(model_input)
        net_effects = self._estimate_net_effects(estimated_effects, costs)
        net_effects["no incentive net effect"] = 0
        net_effects = self._get_recommended_incentive(net_effects) 
        return model_input.join(net_effects)

# COMMAND ----------

from mlflow.models.signature import infer_signature

model_name = "personalized_incentive_recommender"
with mlflow.start_run(run_name=f"{model_name}_run") as experiment_run:    
    personalizedIncentiveRecommender = PersonalizedIncentiveRecommender(
        models_dictionary={
          "tech support": tech_support_total_effect_estimate._estimator_object,
          "discount": discount_effect_estimate._estimator_object,
        },
        effect_modifiers=["Size", "Global Flag"],
    )
    mlflow.pyfunc.log_model(
        artifact_path="model",
        python_model=personalizedIncentiveRecommender,
        signature=infer_signature(input_df, personalizedIncentiveRecommender.predict({},input_df))
    )

model_details = mlflow.register_model(
    model_uri=f"runs:/{experiment_run.info.run_id}/model",
    name=model_name,
)

displayHTML(f"<h1>Model '{model_details.name}' registered</h1>")
displayHTML(f"<h2>-Version {model_details.version}</h2>")

# COMMAND ----------

# MAGIC %md Here we are loading our estimators from MLflow using ```mlflow.pyfunc.load_model``` method. 

# COMMAND ----------

loaded_model = mlflow.pyfunc.load_model(
    f"models:/{model_details.name}/{model_details.version}"
)

final_df = loaded_model.predict(input_df)

display(final_df)

# COMMAND ----------

compare_returns_from_policies(final_df)

# COMMAND ----------


