# Databricks notebook source
# MAGIC %sh sudo apt-get update

# COMMAND ----------

# MAGIC %sh sudo apt-get install -y graphviz libgraphviz-dev 

# COMMAND ----------

# MAGIC %pip install pygraphviz

# COMMAND ----------

# MAGIC %pip install networkx

# COMMAND ----------

# MAGIC %pip install dowhy

# COMMAND ----------

# MAGIC %pip install causal-learn

# COMMAND ----------

# MAGIC %pip install econml

# COMMAND ----------

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.special import expit, logit

# COMMAND ----------

# MAGIC %md
# MAGIC # Data Description and Causal Graph
# MAGIC
# MAGIC (graph generation code commented out to avoid potential issues with dependencies)

# COMMAND ----------

# MAGIC %md
# MAGIC We create one outcome of interest:
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Revenue** | continuous | \\$ Annual revenue from customer given by the amount of software purchased
# MAGIC
# MAGIC We consider three possible treatments, the interventions whose impact we wish to measure:
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Tech Support** | binary | whether the customer received tech support during the year
# MAGIC **Discount** | binary | whether the customer was given a discount during the year
# MAGIC **New Strategy** | binary | whether the customer was targeted for a new engagement strategy with different outreach behaviors
# MAGIC
# MAGIC Finally, we consider a variety of additional customer characteristics that may affect revenue. Including these types of features is crucial for causal analysis in order to map the full causal graph and separate the true effects of treatments on outcomes from other correlation generated by other influences. 
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Global Flag** | binary | whether the customer has global offices
# MAGIC **Major Flag** | binary | whether the customer is a large consumer in their industry
# MAGIC **SMC Flag** | binary | whether the customer is a Small Medium Corporation (as opposed to large corporation)
# MAGIC **Commercial Flag** | binary | whether the customer's business is commercial (as opposed to public secor)
# MAGIC **Planning Summit** | binary | whether a sales team member held an outreach event with the customer during the year
# MAGIC **New Product Adoption** | binary | whether the customer signed a contract for any new products during the year
# MAGIC **IT Spend** | continuous | \\$ spent on IT-related purchases 
# MAGIC **Employee Count** | continuous | number of employees
# MAGIC **PC Count** | continuous | number of PCs used by the customer
# MAGIC **Size** | continuous | customer's total revenue in the previous calendar year
# MAGIC
# MAGIC In simulating the data, we maintain some key characteristics of the data from the real company example, including some correlation patterns between features and some potentially difficult data characteristics, such as large outliers.

# COMMAND ----------

# MAGIC %md
# MAGIC # Data Generation

# COMMAND ----------

# MAGIC %md
# MAGIC #### Generate covariates W, X.
# MAGIC
# MAGIC Most features are independent but some are correlated.

# COMMAND ----------

np.random.seed(1)

n = 10000

global_flag = np.random.binomial(n = 1, p = 0.2, size = n)
major_flag = np.random.binomial(n = 1, p = 0.2, size = n)
smc_flag = np.random.binomial(n = 1, p = 0.5, size = n)
commercial_flag = np.random.binomial(n = 1, p = 0.7, size = n)

size = np.random.exponential(scale = 100000, size = n) + np.random.uniform(low = 5000, high = 15000, size = n)
it_spend = np.exp(np.log(size) - 1.4 + np.random.uniform(size = n))

pc_count = np.random.exponential(scale = 50, size = n) + np.random.uniform(low = 5, high = 10, size = n)
employee_count = np.exp(np.log(pc_count)*0.9 + 0.4 + np.random.uniform(size = n))

size = size.astype(int)
it_spend = it_spend.astype(int)
pc_count = pc_count.astype(int)
employee_count = employee_count.astype(int)


new_X = pd.DataFrame(
    {
        'Global Flag': global_flag,
        'Major Flag': major_flag,
        'SMC Flag': smc_flag,
        'Commercial Flag': commercial_flag,
        'IT Spend': it_spend,
        'Employee Count': employee_count,
        'PC Count': pc_count,
        'Size': size
    }
)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Generate treatment from covariates

# COMMAND ----------

# controls
W_cols = ['Major Flag', 'SMC Flag', 'Commercial Flag', 'IT Spend', 'Employee Count', 'PC Count']

# Tech Support
coefs_W_tech = np.array([0, 0, 0, 0.00001, 0, 0])
const_tech = -0.465
noise_tech = np.random.normal(scale=2.0, size = n)
z_tech = new_X[W_cols] @ coefs_W_tech + const_tech + noise_tech
prob_tech = expit(z_tech)
tech_support = np.random.binomial(n = 1, p = prob_tech, size = n)

# Discount
coefs_W_discount = np.array([0.2, 0, 0, 0.000005, 0, 0])
const_discount = -0.27
noise_discount = np.random.normal(scale=1.5, size = n)
z_discount = new_X[W_cols] @ coefs_W_discount + const_discount + noise_discount
prob_discount = expit(z_discount)
discount = np.random.binomial(n = 1, p = prob_discount, size = n)

# New Engagement Strategy
coefs_W_t3 = np.array([0.5, 0.1, -0.2, 0, 0.005, -0.005])
const_t3 = -0.12
noise_t3 = np.random.normal(scale=1.0, size = n)
z_t3 = new_X[W_cols] @ coefs_W_t3 + const_t3 + noise_t3
prob_t3 = expit(z_t3)
t3 = np.random.binomial(n = 1, p = prob_t3, size = n)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Mediator
# MAGIC
# MAGIC generated from Tech Support

# COMMAND ----------

z_m = tech_support*2-1 + np.random.normal(size=n)
prob_m = expit(z_m)
m = np.random.binomial(n = 1, p = prob_m, size = n)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Outcome

# COMMAND ----------

# X features determine heterogeneous treatment effects
X_cols = ['Global Flag', 'Size']
theta_coef_tech_support = [500, 0.02]
theta_const_tech_support = 5000
te_tech_support = new_X[X_cols] @ theta_coef_tech_support + theta_const_tech_support

theta_coef_discount = [-1000, 0.05]
theta_const_discount = 0
te_discount = new_X[X_cols] @ theta_coef_discount + theta_const_discount

theta_coef_t3 = [0, 0]
theta_const_t3 = 0
te_t3 = new_X[X_cols] @ theta_coef_t3 + theta_const_t3

y_te = te_tech_support*tech_support + te_discount*discount + te_t3*t3

g_coefs = np.array([2000, 0, 5000, 0.25, 0.0001, 0.0001])
g_const = 5000
g_y = new_X[W_cols] @ g_coefs + g_const

y_noise = np.random.normal(scale = 1000, size = n)

mediator_effect = 2000*m

y = pd.Series(y_te + g_y + y_noise + mediator_effect)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Collider
# MAGIC
# MAGIC Caused by both outcome and New Engagement Strategy

# COMMAND ----------

z_c = 0.03*y + 1000*t3 - 1400
prob_c = expit(z_c)
c = np.random.binomial(n = 1, p = prob_c, size = n)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Consolidate

# COMMAND ----------

new_df = (
    pd.concat(
        [
            new_X,
            pd.DataFrame(
                {
                    'Tech Support': tech_support,
                    'Discount': discount,
                    'New Engagement Strategy': t3,
                    'New Product Adoption': m,
                    'Planning Summit': c,
                    'Revenue': y,
                    'Direct Treatment Effect: Tech Support': te_tech_support,
                    'Total Treatment Effect: Tech Support': np.round(te_tech_support + (expit(1) - expit(-1))*2000, decimals=2), # incorporate effect from mediator into total effect.
                    'Direct Treatment Effect: Discount': te_discount,
                    'Total Treatment Effect: Discount': te_discount,
                    'Direct Treatment Effect: New Engagement Strategy': te_t3,
                    'Total Treatment Effect: New Engagement Strategy': te_t3,
                }
            )
        ],
        axis = 1,
    )
    .assign(Revenue = lambda df: df['Revenue'].round(2))
)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Ground Truth ATE check 

# COMMAND ----------

new_df.filter(like='Treatment Effect').mean(axis=0)

# COMMAND ----------

import networkx as nx
import dowhy.gcm

ynode = "Revenue"
mednode = "New Product Adoption"
collider = "Planning Summit"
T_cols = ["Tech Support", "Discount", "New Engagement Strategy"]
trueg = nx.DiGraph()
trueg.nodes = new_df.loc[:, "Global Flag":"Revenue"].columns
trueg.add_edges_from([(w, "Revenue") for w in W_cols])
trueg.add_edges_from([(x, "Revenue") for x in X_cols])  # effect modifiers
for t in T_cols:
    trueg.add_edges_from([(w, t) for w in W_cols])
    if (
        new_df[f"Direct Treatment Effect: {t}"].mean(axis=0) != 0
        and new_df[f"Total Treatment Effect: {t}"].mean(axis=0) != 0
    ):
        trueg.add_edge(t, ynode)
trueg.add_edge(T_cols[0], mednode)
trueg.add_edge(mednode, ynode)  # mediator
trueg.add_edge(T_cols[2], collider)
trueg.add_edge(ynode, collider)
# collider

dowhy.gcm.util.plot(trueg, figure_size=(20, 20))

# COMMAND ----------

# MAGIC %md
# MAGIC #Causal DAG discovery

# COMMAND ----------

from causallearn.search.ConstraintBased.PC import pc

raw_df = new_df.iloc[:,0:14]
# default parameters
cg = pc(np.vstack(raw_df.to_numpy()), node_names=raw_df.columns)

# visualization using pydot
cg.draw_pydot_graph()

# COMMAND ----------

# Adding directions to detected relations
cg.G.add_directed_edge(node1=cg.G.get_node("Size"), node2=cg.G.get_node("IT Spend"))
cg.G.add_directed_edge(node1=cg.G.get_node("IT Spend"), node2=cg.G.get_node("Discount"))
cg.G.add_directed_edge(
    node1=cg.G.get_node("IT Spend"), node2=cg.G.get_node("Tech Support")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("SMC Flag"), node2=cg.G.get_node("New Engagement Strategy")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Tech Support"), node2=cg.G.get_node("New Product Adoption")
)

# Correcting directions
cg.G.remove_connecting_edge(
    node1=cg.G.get_node("Revenue"), node2=cg.G.get_node("Commercial Flag")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Commercial Flag"), node2=cg.G.get_node("Revenue")
)
cg.G.remove_connecting_edge(
    node1=cg.G.get_node("New Engagement Strategy"),
    node2=cg.G.get_node("Commercial Flag"),
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Commercial Flag"),
    node2=cg.G.get_node("New Engagement Strategy"),
)

# Domain Knowledge derived derected relations
cg.G.add_directed_edge(
    node1=cg.G.get_node("Global Flag"), node2=cg.G.get_node("Revenue")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Major Flag"), node2=cg.G.get_node("Revenue")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Employee Count"), node2=cg.G.get_node("PC Count")
)

#Add effect from all basic characteristics to incentives
incentives = ["Discount", "Tech Support", "New Engagement Strategy"]
account_basic_characteristics = [
    "Major Flag",
    "SMC Flag",
    "Commercial Flag",
    "IT Spend",
    "Employee Count",
    "PC Count",
]
for incentive in incentives:
    for characteristic in account_basic_characteristics:
        cg.G.add_directed_edge(
            node1=cg.G.get_node(characteristic), node2=cg.G.get_node(incentive)
        )

cg.draw_pydot_graph()

# COMMAND ----------

import pydot
from causallearn.graph.Edge import Edge
from causallearn.graph.Edges import Edges
from causallearn.graph.Endpoint import Endpoint
from causallearn.graph.Graph import Graph
from causallearn.graph.Node import Node
from causallearn.graph.NodeType import NodeType

def to_pydot(G, labels, dpi = 200):
    nodes = G.get_nodes()
    if labels is not None:
        assert len(labels) == len(nodes)

    pydot_g = pydot.Dot("", graph_type="digraph", fontsize=18)
    pydot_g.obj_dict["attributes"]["dpi"] = dpi
    nodes = G.get_nodes()
    for i, node in enumerate(nodes):
      node_name = labels[i] if labels is not None else node.get_name()
      pydot_g.add_node(pydot.Node(labels[i], label=node.get_name()))
      pydot_g.add_node(pydot.Node(labels[i], label=node_name))

    def get_g_arrow_type(endpoint):
      if endpoint == Endpoint.TAIL:
          return 'none'
      elif endpoint == Endpoint.ARROW:
          return 'normal'
      elif endpoint == Endpoint.CIRCLE:
          return 'odot'
      else:
          raise NotImplementedError()

    edges = G.get_graph_edges()

    for edge in edges:
        node1 = edge.get_node1()
        node2 = edge.get_node2()
        node1_id = nodes.index(node1)
        node2_id = nodes.index(node2)
        dot_edge = pydot.Edge(labels[node1_id], labels[node2_id], dir='both', arrowtail=get_g_arrow_type(edge.get_endpoint1()),
                              arrowhead=get_g_arrow_type(edge.get_endpoint2()))

        pydot_g.add_edge(dot_edge)
    return pydot_g

pdot = to_pydot(cg.G, labels=raw_df.columns)

graph = pdot.to_string().replace("\n", " ") 

# COMMAND ----------

# Generic ML imports
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, Lasso

# EconML imports
from econml.dml import LinearDML
from econml.cate_interpreter import SingleTreePolicyInterpreter

# transformer that performs standard scaling on non-binary variables
ct = ColumnTransformer(
    [
        (
            'num_transformer', 
            StandardScaler(), 
            lambda df: pd.DataFrame(df).apply(pd.Series.nunique).loc[lambda df: df>2].index.tolist()
        )
    ], remainder='passthrough')

model_t = make_pipeline(ct, LogisticRegression(C=1300, max_iter=1000)) # model used to predict treatment
model_y = make_pipeline(ct, Lasso(alpha=20)) # model used to predict outcome

# COMMAND ----------

import dowhy
from causallearn.utils.GraphUtils import GraphUtils

tech_support_effect_model = dowhy.CausalModel(data=raw_df,
                     graph=graph,
                     treatment="Tech Support", 
                     effect_modifiers=["Size", "Global Flag"], 
                     outcome="Revenue"
                     )

tech_support_total_effect_identified_estimand = tech_support_effect_model.identify_effect(
    estimand_type="nonparametric-ate",
    method_name="maximal-adjustment",
)
print(tech_support_total_effect_identified_estimand) 

# COMMAND ----------


# estimate
tech_support_total_effect_estimate = tech_support_effect_model.estimate_effect(
    tech_support_total_effect_identified_estimand,
    method_name="backdoor.econml.dml.LinearDML",
    method_params={
        "init_params": {
            "model_t": model_t,
            "model_y": model_y,
            "linear_first_stages": True,
            "discrete_treatment": True,
            "cv": 3,
            "mc_iters": 1,
        },
    },
)

tech_support_total_effect_estimate.interpret()

# COMMAND ----------

tech_support_direct_effect_identified_estimand = tech_support_effect_model.identify_effect(
    estimand_type="nonparametric-cde",
    method_name="maximal-adjustment",
)

# estimate
tech_support_direct_effect_estimate = tech_support_effect_model.estimate_effect(
    tech_support_direct_effect_identified_estimand,
    method_name="backdoor.econml.dml.LinearDML",
    method_params={
        "init_params": {
            "model_t": model_t,
            "model_y": model_y,
            "linear_first_stages": True,
            "discrete_treatment": True,
            "cv": 3,
            "mc_iters": 1,
        },
    },
)

tech_support_direct_effect_estimate.interpret()

# COMMAND ----------

import dowhy

discount_effect_model = dowhy.CausalModel(data=raw_df,
                     graph=graph,
                     treatment="Discount", 
                     effect_modifiers=["Size", "Global Flag"], 
                     outcome="Revenue"
                     )

identified_discount_effect_estimand = discount_effect_model.identify_effect(
    estimand_type="nonparametric-ate",
    method_name="maximal-adjustment",
)

print(identified_discount_effect_estimand)

# COMMAND ----------

# estimate
discount_effect_estimate = discount_effect_model.estimate_effect(
    identified_discount_effect_estimand,
    confidence_intervals=True,
    method_name="backdoor.econml.dml.LinearDML",
    method_params={
        "init_params": {
            "model_t": model_t,
            "model_y": model_y,
            "linear_first_stages": True,
            "discrete_treatment": True,
            "cv": 3,
            "mc_iters": 1,
        },
    },
)

discount_effect_estimate.interpret()

# COMMAND ----------

import dowhy
from causallearn.utils.GraphUtils import GraphUtils

new_strategy_effect_model = dowhy.CausalModel(
    data=raw_df, graph=graph, treatment="New Engagement Strategy", outcome="Revenue"
)

new_strategy_effect_identified_estimand = new_strategy_effect_model.identify_effect(
    proceed_when_unidentifiable=True
)
print(new_strategy_effect_identified_estimand)

# COMMAND ----------

new_strategy_effect_estimate = new_strategy_effect_model.estimate_effect(
    new_strategy_effect_identified_estimand,
    method_name="backdoor.propensity_score_matching",
    target_units="att",
)
new_strategy_effect_estimate.value

# COMMAND ----------

import functools

estimates_df = pd.DataFrame(
    {
      "Estimated Direct Treatment Effect: Tech Support":[tech_support_direct_effect_estimate.value],
      "Estimated Total Treatment Effect: Tech Support":[tech_support_total_effect_estimate.value],
      "Estimated Total Treatment Effect: Discount":[discount_effect_estimate.value],
      "Estimated Total Treatment Effect: New Engagement Strategy":[new_strategy_effect_estimate.value],
    }
)
ground_truth_df = pd.DataFrame(
    new_df[
        [
            "Direct Treatment Effect: Tech Support",
            "Total Treatment Effect: Tech Support",
            "Total Treatment Effect: Discount",
            "Total Treatment Effect: New Engagement Strategy",
        ]
    ].mean()
).T

comparison_df = pd.concat([ground_truth_df, estimates_df], axis=1)

comparison_df[
    functools.reduce(
        lambda acc, x: acc + [x[0], x[1]],
        [
            [ground_truth, estimate]
            for ground_truth, estimate in zip(
                ground_truth_df.columns, estimates_df.columns
            )
        ],
    )
]


# COMMAND ----------

res_random = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    method_name="random_common_cause",
    num_simulations=20,
    n_jobs=2,
)

res_unobserved = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    method_name="add_unobserved_common_cause",
    confounders_effect_on_treatment="binary_flip",
    confounders_effect_on_outcome="linear",
    effect_fraction_on_treatment=0.05,
    effect_fraction_on_outcome=0.05,
)

res_placebo = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    method_name="placebo_treatment_refuter",
    placebo_type="permute",
    num_simulations=20,
    n_jobs=2,
)

res_subset = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    method_name="data_subset_refuter",
    subset_fraction=0.8,
    num_simulations=20,
    n_jobs=2,
)


coefficients = np.array([10, 0.02])
bias = 1000


def linear_gen(df):
    y_new = np.dot(df[["W0", "W1"]].values, coefficients) + bias
    return y_new


ref = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    method_name="dummy_outcome_refuter",
    outcome_function=linear_gen,
)
res_dummy_outcome = ref[0]

# COMMAND ----------

refutation_df = pd.DataFrame()
for refutation_result in [res_random, res_unobserved, res_placebo, res_subset, ref[0]]:
    
    row = {
        'Refutation Type': refutation_result.refutation_type,
        'Estimated Effect': refutation_result.estimated_effect,
        'New Effect': refutation_result.new_effect,
        'Refutation Result (p value)': refutation_result.refutation_result['p_value'] if refutation_result.refutation_result else None,
    }
    
    refutation_df = refutation_df.append(row, ignore_index = True)
refutation_df

# COMMAND ----------


