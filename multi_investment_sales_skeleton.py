# Databricks notebook source
# MAGIC %sh sudo apt-get update

# COMMAND ----------

# MAGIC %sh sudo apt-get install -y graphviz libgraphviz-dev 

# COMMAND ----------

# MAGIC %pip install pygraphviz --quiet

# COMMAND ----------

# MAGIC %pip install networkx --quiet

# COMMAND ----------

# MAGIC %pip install dowhy --quiet

# COMMAND ----------

# MAGIC %pip install causal-learn --quiet

# COMMAND ----------

# MAGIC %pip install econml --quiet

# COMMAND ----------

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import pickle as pkl
import os
from scipy.special import expit, logit

# COMMAND ----------

# MAGIC %md
# MAGIC # Data Description and Causal Graph
# MAGIC
# MAGIC (graph generation code commented out to avoid potential issues with dependencies)

# COMMAND ----------

# MAGIC %md
# MAGIC We create one outcome of interest:
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Revenue** | continuous | \\$ Annual revenue from customer given by the amount of software purchased
# MAGIC
# MAGIC We consider three possible treatments, the interventions whose impact we wish to measure:
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Tech Support** | binary | whether the customer received tech support during the year
# MAGIC **Discount** | binary | whether the customer was given a discount during the year
# MAGIC **New Strategy** | binary | whether the customer was targeted for a new engagement strategy with different outreach behaviors
# MAGIC
# MAGIC Finally, we consider a variety of additional customer characteristics that may affect revenue. Including these types of features is crucial for causal analysis in order to map the full causal graph and separate the true effects of treatments on outcomes from other correlation generated by other influences. 
# MAGIC
# MAGIC Feature Name | Type | Details 
# MAGIC :--- |:--- |:--- 
# MAGIC **Global Flag** | binary | whether the customer has global offices
# MAGIC **Major Flag** | binary | whether the customer is a large consumer in their industry
# MAGIC **SMC Flag** | binary | whether the customer is a Small Medium Corporation (as opposed to large corporation)
# MAGIC **Commercial Flag** | binary | whether the customer's business is commercial (as opposed to public secor)
# MAGIC **Planning Summit** | binary | whether a sales team member held an outreach event with the customer during the year
# MAGIC **New Product Adoption** | binary | whether the customer signed a contract for any new products during the year
# MAGIC **IT Spend** | continuous | \\$ spent on IT-related purchases 
# MAGIC **Employee Count** | continuous | number of employees
# MAGIC **PC Count** | continuous | number of PCs used by the customer
# MAGIC **Size** | continuous | customer's total revenue in the previous calendar year
# MAGIC
# MAGIC In simulating the data, we maintain some key characteristics of the data from the real company example, including some correlation patterns between features and some potentially difficult data characteristics, such as large outliers.

# COMMAND ----------

# MAGIC %md
# MAGIC # Data Generation

# COMMAND ----------

# MAGIC %md
# MAGIC #### Generate covariates W, X.
# MAGIC
# MAGIC Most features are independent but some are correlated.

# COMMAND ----------

np.random.seed(1)

n = 10000

global_flag = np.random.binomial(n=1, p=0.2, size=n)
major_flag = np.random.binomial(n=1, p=0.2, size=n)
smc_flag = np.random.binomial(n=1, p=0.5, size=n)
commercial_flag = np.random.binomial(n=1, p=0.7, size=n)

size = np.random.exponential(scale=100000, size=n) + np.random.uniform(
    low=5000, high=15000, size=n
)
it_spend = np.exp(np.log(size) - 1.4 + np.random.uniform(size=n))

# pc_count = np.random.exponential(scale = 50, size = n) + np.random.uniform(low = 5, high = 10, size = n)
# employee_count = np.exp(np.log(pc_count)*0.9 + 0.4 + np.random.uniform(size = n))

employee_count = np.exp(
    np.log(
        np.random.exponential(scale=50, size=n)
        + np.random.uniform(low=5, high=10, size=n)
    )
    * 0.9
    + 0.4
    + np.random.uniform(size=n)
)
pc_count = np.exp((np.log(employee_count) - np.random.uniform(size=n) - 0.4) / 0.9 )

size = size.astype(int)
it_spend = it_spend.astype(int)
pc_count = pc_count.astype(int)
employee_count = employee_count.astype(int)


new_X = pd.DataFrame(
    {
        "Global Flag": global_flag,
        "Major Flag": major_flag,
        "SMC Flag": smc_flag,
        "Commercial Flag": commercial_flag,
        "IT Spend": it_spend,
        "Employee Count": employee_count,
        "PC Count": pc_count,
        "Size": size,
    }
)

# COMMAND ----------

new_X[new_X["Employee Count"]<new_X["PC Count"]].shape

# COMMAND ----------

# MAGIC %md
# MAGIC #### Generate treatment from covariates

# COMMAND ----------

# controls
W_cols = ['Major Flag', 'SMC Flag', 'Commercial Flag', 'IT Spend', 'Employee Count', 'PC Count']

# Tech Support
coefs_W_tech = np.array([0, 0, 0, 0.00001, 0, 0])
const_tech = -0.465
noise_tech = np.random.normal(scale=2.0, size = n)
z_tech = new_X[W_cols] @ coefs_W_tech + const_tech + noise_tech
prob_tech = expit(z_tech)
tech_support = np.random.binomial(n = 1, p = prob_tech, size = n)

# Discount
coefs_W_discount = np.array([0.2, 0, 0, 0.000005, 0, 0])
const_discount = -0.27
noise_discount = np.random.normal(scale=1.5, size = n)
z_discount = new_X[W_cols] @ coefs_W_discount + const_discount + noise_discount
prob_discount = expit(z_discount)
discount = np.random.binomial(n = 1, p = prob_discount, size = n)

# New Engagement Strategy
coefs_W_t3 = np.array([0.5, 0.1, -0.2, 0, 0.005, -0.005])
const_t3 = -0.12
noise_t3 = np.random.normal(scale=1.0, size = n)
z_t3 = new_X[W_cols] @ coefs_W_t3 + const_t3 + noise_t3
prob_t3 = expit(z_t3)
t3 = np.random.binomial(n = 1, p = prob_t3, size = n)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Mediator
# MAGIC
# MAGIC generated from Tech Support

# COMMAND ----------

z_m = tech_support*2-1 + np.random.normal(size=n)
prob_m = expit(z_m)
m = np.random.binomial(n = 1, p = prob_m, size = n)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Outcome

# COMMAND ----------

# X features determine heterogeneous treatment effects
X_cols = ['Global Flag', 'Size']
theta_coef_tech_support = [500, 0.02]
theta_const_tech_support = 5000
te_tech_support = new_X[X_cols] @ theta_coef_tech_support + theta_const_tech_support

theta_coef_discount = [-1000, 0.05]
theta_const_discount = 0
te_discount = new_X[X_cols] @ theta_coef_discount + theta_const_discount

theta_coef_t3 = [0, 0]
theta_const_t3 = 0
te_t3 = new_X[X_cols] @ theta_coef_t3 + theta_const_t3

y_te = te_tech_support*tech_support + te_discount*discount + te_t3*t3

g_coefs = np.array([2000, 0, 5000, 0.25, 0.0001, 0.0001])
g_const = 5000
g_y = new_X[W_cols] @ g_coefs + g_const

y_noise = np.random.normal(scale = 1000, size = n)

mediator_effect = 2000*m

y = pd.Series(y_te + g_y + y_noise + mediator_effect)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Collider
# MAGIC
# MAGIC Caused by both outcome and New Engagement Strategy

# COMMAND ----------

z_c = 0.03*y + 1000*t3 - 1400
prob_c = expit(z_c)
c = np.random.binomial(n = 1, p = prob_c, size = n)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Consolidate

# COMMAND ----------

new_df = (
    pd.concat(
        [
            new_X,
            pd.DataFrame(
                {
                    'Tech Support': tech_support,
                    'Discount': discount,
                    'New Engagement Strategy': t3,
                    'New Product Adoption': m,
                    'Planning Summit': c,
                    'Revenue': y,
                    'Direct Treatment Effect: Tech Support': te_tech_support,
                    'Total Treatment Effect: Tech Support': np.round(te_tech_support + (expit(1) - expit(-1))*2000, decimals=2), # incorporate effect from mediator into total effect.
                    'Direct Treatment Effect: Discount': te_discount,
                    'Total Treatment Effect: Discount': te_discount,
                    'Direct Treatment Effect: New Engagement Strategy': te_t3,
                    'Total Treatment Effect: New Engagement Strategy': te_t3,
                }
            )
        ],
        axis = 1,
    )
    .assign(Revenue = lambda df: df['Revenue'].round(2))
)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Ground Truth ATE check 

# COMMAND ----------

new_df.filter(like='Treatment Effect').mean(axis=0)

# COMMAND ----------

import networkx as nx
import dowhy.gcm

ynode = "Revenue"
mednode = "New Product Adoption"
collider = "Planning Summit"
T_cols = ["Tech Support", "Discount", "New Engagement Strategy"]
trueg = nx.DiGraph()
trueg.nodes = new_df.loc[:, "Global Flag":"Revenue"].columns
trueg.add_edges_from([(w, "Revenue") for w in W_cols])
trueg.add_edges_from([(x, "Revenue") for x in X_cols])  # effect modifiers
for t in T_cols:
    trueg.add_edges_from([(w, t) for w in W_cols])
    if (
        new_df[f"Direct Treatment Effect: {t}"].mean(axis=0) != 0
        and new_df[f"Total Treatment Effect: {t}"].mean(axis=0) != 0
    ):
        trueg.add_edge(t, ynode)
trueg.add_edge(T_cols[0], mednode)
trueg.add_edge(mednode, ynode)  # mediator
trueg.add_edge(T_cols[2], collider)
trueg.add_edge(ynode, collider)
# collider

trueg.add_edge("Size", "IT Spend")
trueg.add_edge("PC Count", "Employee Count")


dowhy.gcm.util.plot(trueg, figure_size=(20, 20))

# COMMAND ----------

# MAGIC %md
# MAGIC #Causal DAG discovery

# COMMAND ----------

from causallearn.search.ConstraintBased.PC import pc

raw_df = new_df.iloc[:,0:14]
# default parameters
cg = pc(np.vstack(raw_df.to_numpy()), node_names=raw_df.columns)

# visualization using pydot
cg.draw_pydot_graph()

# COMMAND ----------

# Adding directions to detected relations
cg.G.add_directed_edge(node1=cg.G.get_node("Size"), node2=cg.G.get_node("IT Spend"))
cg.G.add_directed_edge(node1=cg.G.get_node("IT Spend"), node2=cg.G.get_node("Discount"))
cg.G.add_directed_edge(
    node1=cg.G.get_node("IT Spend"), node2=cg.G.get_node("Tech Support")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("SMC Flag"), node2=cg.G.get_node("New Engagement Strategy")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Tech Support"), node2=cg.G.get_node("New Product Adoption")
)

# Correcting directions
cg.G.remove_connecting_edge(
    node1=cg.G.get_node("Revenue"), node2=cg.G.get_node("Commercial Flag")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Commercial Flag"), node2=cg.G.get_node("Revenue")
)
cg.G.remove_connecting_edge(
    node1=cg.G.get_node("New Engagement Strategy"),
    node2=cg.G.get_node("Commercial Flag"),
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Commercial Flag"),
    node2=cg.G.get_node("New Engagement Strategy"),
)

# Domain Knowledge derived derected relations
cg.G.add_directed_edge(
    node1=cg.G.get_node("Global Flag"), node2=cg.G.get_node("Revenue")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Major Flag"), node2=cg.G.get_node("Revenue")
)
cg.G.add_directed_edge(
    node1=cg.G.get_node("Employee Count"), node2=cg.G.get_node("PC Count")
)

#Add effect from all basic characteristics to incentives
incentives = ["Discount", "Tech Support", "New Engagement Strategy"]
account_basic_characteristics = [
    "Major Flag",
    "SMC Flag",
    "Commercial Flag",
    "IT Spend",
    "Employee Count",
    "PC Count",
]
for incentive in incentives:
    for characteristic in account_basic_characteristics:
        cg.G.add_directed_edge(
            node1=cg.G.get_node(characteristic), node2=cg.G.get_node(incentive)
        )

cg.draw_pydot_graph()

# COMMAND ----------

import pydot
from causallearn.graph.Edge import Edge
from causallearn.graph.Edges import Edges
from causallearn.graph.Endpoint import Endpoint
from causallearn.graph.Graph import Graph
from causallearn.graph.Node import Node
from causallearn.graph.NodeType import NodeType

def to_pydot(G, labels, dpi = 200):
    nodes = G.get_nodes()
    if labels is not None:
        assert len(labels) == len(nodes)

    pydot_g = pydot.Dot("", graph_type="digraph", fontsize=18)
    pydot_g.obj_dict["attributes"]["dpi"] = dpi
    nodes = G.get_nodes()
    for i, node in enumerate(nodes):
      node_name = labels[i] if labels is not None else node.get_name()
      pydot_g.add_node(pydot.Node(labels[i], label=node.get_name()))
      pydot_g.add_node(pydot.Node(labels[i], label=node_name))

    def get_g_arrow_type(endpoint):
      if endpoint == Endpoint.TAIL:
          return 'none'
      elif endpoint == Endpoint.ARROW:
          return 'normal'
      elif endpoint == Endpoint.CIRCLE:
          return 'odot'
      else:
          raise NotImplementedError()

    edges = G.get_graph_edges()

    for edge in edges:
        node1 = edge.get_node1()
        node2 = edge.get_node2()
        node1_id = nodes.index(node1)
        node2_id = nodes.index(node2)
        dot_edge = pydot.Edge(labels[node1_id], labels[node2_id], dir='both', arrowtail=get_g_arrow_type(edge.get_endpoint1()),
                              arrowhead=get_g_arrow_type(edge.get_endpoint2()))

        pydot_g.add_edge(dot_edge)
    return pydot_g

pdot = to_pydot(cg.G, labels=raw_df.columns)

graph = pdot.to_string().replace("\n", " ") 

# COMMAND ----------

# Generic ML imports
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, Lasso

# EconML imports
from econml.dml import LinearDML
from econml.cate_interpreter import SingleTreePolicyInterpreter

# transformer that performs standard scaling on non-binary variables
ct = ColumnTransformer(
    [
        (
            'num_transformer', 
            StandardScaler(), 
            lambda df: pd.DataFrame(df).apply(pd.Series.nunique).loc[lambda df: df>2].index.tolist()
        )
    ], remainder='passthrough')

model_t = make_pipeline(ct, LogisticRegression(C=1300, max_iter=1000)) # model used to predict treatment
model_y = make_pipeline(ct, Lasso(alpha=20)) # model used to predict outcome

# COMMAND ----------

import dowhy

tech_support_effect_model = dowhy.CausalModel(data=raw_df,
                     graph=graph,
                     treatment="Tech Support", 
                     outcome="Revenue"
                     )

tech_support_total_effect_identified_estimand = tech_support_effect_model.identify_effect(
    estimand_type="nonparametric-ate",
    method_name="maximal-adjustment",
)
print(tech_support_total_effect_identified_estimand) 

# COMMAND ----------

# MAGIC %md
# MAGIC The following code creates a wrapper function, EstimatorWrapper, that uses the const_marginal_effect method to return the point estimates of the effect given the effect modifiers. We will wrap our effect estimator we get from EconML with this function and log it in MLflow. Later when we make a batch inference, we load this model (estimator) from MLflow and use it for inference.   

# COMMAND ----------

import mlflow
class EstimatorWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model):
    self.model = model
    
  def predict(self, context, model_input):
    return self.model.const_marginal_effect(model_input)

# COMMAND ----------

# MAGIC %md
# MAGIC The custom fuction logs an identified estimand, various parameters and models to MLflow.

# COMMAND ----------

def log_results(effect, estimand, effect_modifiers, method_name, init_params, wrappedEstimator):
  mlflow.set_tags({"effect": effect})

  with open("estimand.pkl", 'wb') as output:
    pkl.dump(estimand, output, pkl.HIGHEST_PROTOCOL)
    mlflow.log_artifact("estimand.pkl")
    
  mlflow.log_param("effect_modifiers", effect_modifiers)
  mlflow.log_param("method_name", method_name)
  for key, value in init_params.items():
    if 'model' in key:
      mlflow.sklearn.log_model(value, key)
    else:
      mlflow.log_param(key, value)

  return mlflow.pyfunc.log_model("effect_estimator", python_model=wrappedEstimator)

# COMMAND ----------

with mlflow.start_run(run_name="tech_support_total_effect_estimator"):
  effect_modifiers = ["Size", "Global Flag"]
  method_name = "backdoor.econml.dml.LinearDML"
  init_params = {
    "model_t": model_t,
    "model_y": model_y,
    "linear_first_stages": True,
    "discrete_treatment": True,
    "cv": 3,
    "mc_iters": 1,
  }

  tech_support_total_effect_estimate = tech_support_effect_model.estimate_effect(
      tech_support_total_effect_identified_estimand,
      effect_modifiers=effect_modifiers, 
      method_name=method_name,
      method_params={"init_params": init_params},
  )

  tech_support_total_effect_estimate.interpret()
  wrappedEstimator = EstimatorWrapper(tech_support_total_effect_estimate._estimator_object)

  tech_support_total_effect_estimator_info = log_results("tech_support_total_effect", 
                                                          tech_support_total_effect_identified_estimand,
                                                          effect_modifiers,
                                                          method_name,
                                                          init_params,
                                                          wrappedEstimator)


# COMMAND ----------

tech_support_direct_effect_identified_estimand = tech_support_effect_model.identify_effect(
    estimand_type="nonparametric-cde",
    method_name="maximal-adjustment",
)
print(tech_support_direct_effect_identified_estimand) 

# COMMAND ----------

with mlflow.start_run(run_name="tech_support_direct_effect_estimator"):
  effect_modifiers = ["Size", "Global Flag"]
  method_name = "backdoor.econml.dml.LinearDML"
  init_params = {
    "model_t": model_t,
    "model_y": model_y,
    "linear_first_stages": True,
    "discrete_treatment": True,
    "cv": 3,
    "mc_iters": 1,
  }
  
  tech_support_direct_effect_estimate = tech_support_effect_model.estimate_effect(
      tech_support_direct_effect_identified_estimand,
      effect_modifiers=effect_modifiers, 
      method_name=method_name,
      method_params={"init_params": init_params},
  )

  tech_support_direct_effect_estimate.interpret()
  wrappedEstimator = EstimatorWrapper(tech_support_direct_effect_estimate._estimator_object)
  tech_support_direct_effect_estimator_info = log_results("tech_support_direct_effect", 
                                                          tech_support_direct_effect_identified_estimand,
                                                          effect_modifiers,
                                                          method_name,
                                                          init_params,
                                                          wrappedEstimator)

# COMMAND ----------

discount_effect_model = dowhy.CausalModel(data=raw_df,
                     graph=graph,
                     treatment="Discount", 
                     outcome="Revenue"
                     )

discount_effect_identified_estimand = discount_effect_model.identify_effect(
    estimand_type="nonparametric-ate",
    method_name="maximal-adjustment",
)

print(discount_effect_identified_estimand)

# COMMAND ----------

with mlflow.start_run(run_name="discount_effect_estimator"):
  effect_modifiers = ["Size", "Global Flag"]
  method_name = "backdoor.econml.dml.LinearDML"
  init_params = {
    "model_t": model_t,
    "model_y": model_y,
    "linear_first_stages": True,
    "discrete_treatment": True,
    "cv": 3,
    "mc_iters": 10,
  }
  
  discount_effect_estimate = discount_effect_model.estimate_effect(
      discount_effect_identified_estimand, 
      confidence_intervals=True,
      effect_modifiers=effect_modifiers,
      method_name=method_name,
      method_params={"init_params": init_params},
  )

  discount_effect_estimate.interpret()

  wrappedEstimator = EstimatorWrapper(discount_effect_estimate._estimator_object)
  discount_effect_estimator_info = log_results("discount_effect", 
                                                discount_effect_identified_estimand,
                                                effect_modifiers,
                                                method_name,
                                                init_params,
                                                wrappedEstimator)


# COMMAND ----------

import dowhy

new_strategy_effect_model = dowhy.CausalModel(
    data=raw_df, 
    graph=graph, 
    treatment="New Engagement Strategy", 
    outcome="Revenue"
)

new_strategy_effect_identified_estimand = new_strategy_effect_model.identify_effect(
    proceed_when_unidentifiable=True
)
print(new_strategy_effect_identified_estimand)

# COMMAND ----------

new_strategy_effect_estimate = new_strategy_effect_model.estimate_effect(
    new_strategy_effect_identified_estimand,
    method_name="backdoor.propensity_score_matching",
    target_units="att",
)
new_strategy_effect_estimate.value

# COMMAND ----------

import functools

estimates_df = pd.DataFrame(
    {
      "Estimated Direct Treatment Effect: Tech Support":[tech_support_direct_effect_estimate.value],
      "Estimated Total Treatment Effect: Tech Support":[tech_support_total_effect_estimate.value],
      "Estimated Total Treatment Effect: Discount":[discount_effect_estimate.value],
      "Estimated Total Treatment Effect: New Engagement Strategy":[new_strategy_effect_estimate.value],
    }
)
ground_truth_df = pd.DataFrame(
    new_df[
        [
            "Direct Treatment Effect: Tech Support",
            "Total Treatment Effect: Tech Support",
            "Total Treatment Effect: Discount",
            "Total Treatment Effect: New Engagement Strategy",
        ]
    ].mean()
).T

comparison_df = pd.concat([ground_truth_df, estimates_df], axis=1)

comparison_df[
    functools.reduce(
        lambda acc, x: acc + [x[0], x[1]],
        [
            [ground_truth, estimate]
            for ground_truth, estimate in zip(
                ground_truth_df.columns, estimates_df.columns
            )
        ],
    )
]


# COMMAND ----------

res_random_common_cause = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="random_common_cause",
    num_simulations=1000,
    n_jobs=16,
)

refutation_random_common_cause_df = pd.DataFrame([{
        "Refutation Type": res_random_common_cause.refutation_type,
        "Estimated Effect": res_random_common_cause.estimated_effect,
        "New Effect": res_random_common_cause.new_effect,
        "Refutation Result (p value)": res_random_common_cause.refutation_result["p_value"] 
    }])

refutation_random_common_cause_df  

# COMMAND ----------

mlflow.autolog(disable=True)

res_unobserved_common_cause = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="add_unobserved_common_cause",
    confounders_effect_on_treatment="binary_flip",
    confounders_effect_on_outcome="linear",
    effect_fraction_on_treatment=0.05,
    effect_fraction_on_outcome=0.05,
)

refutation_unobserved_common_cause_df = pd.DataFrame([{
        "Refutation Type": res_unobserved_common_cause.refutation_type,
        "Estimated Effect": res_unobserved_common_cause.estimated_effect,
        "New Effect": res_unobserved_common_cause.new_effect,
        "Refutation Result (p value)": None 
    }])

refutation_unobserved_common_cause_df  

# COMMAND ----------

res_placebo = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="placebo_treatment_refuter",
    placebo_type="permute",
    num_simulations=1000,
    n_jobs=16,
)

refutation_placebo_df = pd.DataFrame([{
        "Refutation Type": res_placebo.refutation_type,
        "Estimated Effect": res_placebo.estimated_effect,
        "New Effect": res_placebo.new_effect,
        "Refutation Result (p value)": res_placebo.refutation_result["p_value"] 
    }])

refutation_placebo_df  

# COMMAND ----------

res_subset = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="data_subset_refuter",
    subset_fraction=0.8,
    num_simulations=1000,
    n_jobs=16,
)

refutation_subset_df = pd.DataFrame([{
        "Refutation Type": res_subset.refutation_type,
        "Estimated Effect": res_subset.estimated_effect,
        "New Effect": res_subset.new_effect,
        "Refutation Result (p value)": res_subset.refutation_result["p_value"] 
    }])

refutation_subset_df  

# COMMAND ----------

mlflow.autolog(disable=True)

coefficients = np.array([10, 0.02])
bias = 1000


def linear_gen(df):
    y_new = np.dot(df[["W0", "W1"]].values, coefficients) + bias
    return y_new


res_dummy_outcome = tech_support_effect_model.refute_estimate(
    tech_support_direct_effect_identified_estimand,
    tech_support_direct_effect_estimate,
    show_progress_bar=True,
    method_name="dummy_outcome_refuter",
    outcome_function=linear_gen,
)[0]

refutation_dummy_outcome_df = pd.DataFrame([{
        "Refutation Type": res_dummy_outcome.refutation_type,
        "Estimated Effect": res_dummy_outcome.estimated_effect,
        "New Effect": res_dummy_outcome.new_effect,
        "Refutation Result (p value)": res_dummy_outcome.refutation_result["p_value"] 
    }])

refutation_dummy_outcome_df  

# COMMAND ----------

refutation_df = pd.concat([
        refutation_random_common_cause_df,
        refutation_unobserved_common_cause_df,
        refutation_subset_df,
        refutation_placebo_df,
        refutation_dummy_outcome_df,
    ])
refutation_df

# COMMAND ----------

# MAGIC %md
# MAGIC # Make Policy Recommendations

# COMMAND ----------

# MAGIC %md
# MAGIC In this section, we use EconML tools to visualize differences in conditional average treatment effects across customers and select an optimal investment plan for each customer.
# MAGIC
# MAGIC In order to decide whether to offer each investment to the customer, we need to know the cost of providing the incentive as well as the benefits of doing so. In this step we define a cost function to specify how expensive it would be to provide each kind of incentive to each customer. In other data samples you can define these costs as a function of customer features, upload a matrix of costs, or set constant costs for each treatment (the default is zero). In this example, we set the cost of ```discount``` to be a fix value of $7000 per account, while the cost of ```tech support``` is $100 per PC.

# COMMAND ----------

# Define cost function
def cost_fn_interaction(raw_data):
    t1_cost = raw_data[["PC Count"]].values * 100
    t2_cost = np.ones((raw_data.shape[0], 1)) * 7000
    return np.hstack([t1_cost, t2_cost, t1_cost + t2_cost])

# Transform T to one-dimensional array with consecutive integer encoding
def treat_map(t):
    return np.dot(t, 2 ** np.arange(t.shape[0]))

# COMMAND ----------

# Model wrapper for EconML model 
class EconMLModelWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model):
    self.model = model

# COMMAND ----------

mlflow.autolog(disable=True)
# define input variables for composite treatment model
effect_modifiers = ["Size", "Global Flag"]
treatment_columns = ["Tech Support", "Discount", "New Engagement Strategy"]
outcome = 'Revenue'
collider_column = 'Planning Summit'
X_policy = raw_df[effect_modifiers + ['PC Count']]
W_with_mediator = raw_df.drop(columns = treatment_columns + [outcome] + effect_modifiers + [collider_column])
Y = raw_df[outcome]

with mlflow.start_run(run_name="composite_treatment_model"):

  composite_treatment = raw_df[['Tech Support', 'Discount']].apply(treat_map, axis = 1).rename('Composite Treatment')
  composite_model = LinearDML(
      model_t = model_t,
      model_y = model_y,
      discrete_treatment=True, 
      linear_first_stages=True,
      mc_iters=10
  )
  
  composite_model.fit(Y=Y, T=composite_treatment, X=X_policy, W=W_with_mediator)
  
  mlflow.set_tags({"type": "composite_treatment_model"})
  mlflow.log_param("composite_treatment", ['Tech Support','Discount'])
  mlflow.sklearn.log_model(model_t, "model_t")
  mlflow.sklearn.log_model(model_y, "model_y")
  
  mlflow.pyfunc.log_model("composite_treatment_model", python_model=EconMLModelWrapper(composite_model))


# COMMAND ----------

# MAGIC %md
# MAGIC #### Deriving a Rule for Allocating Investments
# MAGIC
# MAGIC EconML's policy tree interpreter fits a regression tree on the set of conditional average treatment effects estimated earlier. The interpreter divides the sample into groups that all respond similarly to treatments, and recommends the optimal treatment for each group, or leaf. This same rule can be applied to pick treatments for new samples of customers with the same characteristics. Set the depth option to allow the intpreter to create more or fewer groups (a depth of 2 creates a maximum of 4 groups).

# COMMAND ----------

est = SingleTreePolicyInterpreter(random_state=1,
                     min_impurity_decrease=0.1,
                     min_samples_leaf=40,
                     max_depth=2)

est.interpret(composite_model, X_policy, sample_treatment_costs = cost_fn_interaction(raw_df))

# COMMAND ----------

# MAGIC %md
# MAGIC Observe recommended treatment policies

# COMMAND ----------

# MAGIC %matplotlib inline
# MAGIC plt.figure(figsize=(15, 5))
# MAGIC est.plot(treatment_names=['None', 'Tech Support', 'Discount', 'Tech Support and Discount'], feature_names=['Global Flag', 'Size', 'PC Count'])

# COMMAND ----------

# MAGIC %md
# MAGIC #### Individualized policy recommendations 

# COMMAND ----------

# MAGIC %md
# MAGIC For our current sample of customers, we can also identify the best treatment plan for each individual customer based on their CATE. We use the model's `const_marginal_effect` method to find the counterfactual treatment effect for each possible treatment. We then subtract the treatment cost and choose the treatment with the highest return. That is the recommended policy.
# MAGIC
# MAGIC To visualize this output, we plot each customer based on their PC count and past revenue, the most important determinants of treatment according to the tree interpreter, and color code them based on recommended treatment.

# COMMAND ----------

# MAGIC %md Here we are loading our estimators from MLflow using ```mlflow.pyfunc.load_model``` method. 

# COMMAND ----------

estimator_info = [tech_support_direct_effect_estimator_info, discount_effect_estimator_info]
models = [mlflow.pyfunc.load_model(model_uri=info.model_uri) for info in estimator_info]

# COMMAND ----------

effects = np.hstack([model.predict(raw_df[effect_modifiers]) for model in models])
effects_with_interaction = np.hstack([effects, effects.sum(axis=1, keepdims=True)])
net_effects = effects_with_interaction - cost_fn_interaction(raw_df)
net_effects_with_control = np.hstack([np.zeros(shape = (raw_df[effect_modifiers].shape[0], 1)), net_effects])

# COMMAND ----------

recommended_T = net_effects_with_control.argmax(axis = 1) # Data sample

# COMMAND ----------

# MAGIC %matplotlib inline
# MAGIC all_treatments = np.array(['None', 'Tech Support', 'Discount', 'Tech Support and Discount'])
# MAGIC ax1 = sns.scatterplot(
# MAGIC     x=raw_df['Size'],
# MAGIC     y=raw_df["PC Count"],
# MAGIC     hue=all_treatments[recommended_T],
# MAGIC     hue_order=all_treatments,
# MAGIC     cmap="Dark2",
# MAGIC     s=40,
# MAGIC )
# MAGIC plt.legend(title="Investment Policy")
# MAGIC plt.setp(
# MAGIC     ax1,
# MAGIC     xlabel="Customer Size",
# MAGIC     ylabel="PC Count",
# MAGIC     title="Optimal Investment Policy by Customer",
# MAGIC )
# MAGIC plt.show()

# COMMAND ----------


